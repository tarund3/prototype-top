{
  "experiment_name": "ntp_baseline",
  "description": "Next Token Prediction baseline experiment",
  "objectives": ["ntp"],
  "model": {
    "n_layer": 4,
    "n_head": 8,
    "d_model": 256,
    "max_seq_len": 1024
  },
  "training": {
    "seq_len": 512,
    "batch_size": 8,
    "learning_rate": 1e-4,
    "weight_decay": 0.01,
    "max_epochs": 10,
    "grad_clip": 1.0
  },
  "data": {
    "dataset": "wikitext-2",
    "tokenizer": "gpt2",
    "seq_len": 512
  },
  "loss_weights": {
    "lambda_ntp": 1.0,
    "lambda_mtp": 0.0,
    "lambda_top": 0.0
  },
  "hardware": {
    "gpu_type": "RTX 4090",
    "expected_vram_gb": 8.0
  },
  "expected_results": {
    "final_ppl": "~50-55",
    "convergence_epochs": "~8-10"
  }
}
